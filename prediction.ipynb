{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            f\"data/OraclesElixir/{year}_LoL_esports_match_data_from_OraclesElixir.csv\",\n",
    "            dtype={\"url\": \"str\"}\n",
    "        )\n",
    "        for year in range(2020, 2025)\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "rows, cols = df.shape\n",
    "print(f\"The CSV file has {rows} rows and {cols} columns.\")\n",
    "\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for Complete Matches Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_complete_rows = df[df[\"datacompleteness\"] == 'complete'].shape[0]\n",
    "total_rows = df.shape[0]\n",
    "ratio = num_complete_rows / total_rows\n",
    "print(f\"Number of rows where datacompleteness is 'complete': {num_complete_rows}\")\n",
    "print(f\"Ratio of 'complete' rows to total rows: {ratio:.4f}\")\n",
    "\n",
    "df = df[df[\"datacompleteness\"] == 'complete']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Individual Stats to Team-Level Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_rows = df[df['position'] == 'team'].copy()\n",
    "player_rows = df[df['position'] != 'team']\n",
    "\n",
    "positions = ['top', 'jng', 'mid', 'bot', 'sup']\n",
    "\n",
    "for pos in positions:\n",
    "    champ_col = (\n",
    "        player_rows[player_rows['position'] == pos]\n",
    "        .loc[:, ['gameid', 'side', 'champion']]\n",
    "        .rename(columns={'champion': f'{pos}_champ'})\n",
    "    )\n",
    "    \n",
    "    team_rows = team_rows.merge(champ_col, on=['gameid', 'side'], how='left')\n",
    "df = team_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_void = df['void_grubs'].corr(df['result'])  \n",
    "print(\"Correlation with void:\", correlation_void)\n",
    "\n",
    "correlation_monsterkillsownjungle = df['monsterkillsownjungle'].corr(df['result'])  \n",
    "print(\"Correlation with monsterkillsownjungle:\", correlation_monsterkillsownjungle)\n",
    "\n",
    "correlation_turretplates = df['turretplates'].corr(df['result'])  \n",
    "print(\"Correlation with turretplates:\", correlation_turretplates)\n",
    "\n",
    "correlation_heralds = df['heralds'].corr(df['result'])  \n",
    "print(\"Correlation with heralds:\", correlation_heralds)\n",
    "\n",
    "correlation_visionscore = df['visionscore'].corr(df['result'])  \n",
    "print(\"Correlation with visionscore:\", correlation_visionscore)\n",
    "\n",
    "correlation_vspm = df['vspm'].corr(df['result'])  \n",
    "print(\"Correlation with vspm:\", correlation_vspm)\n",
    "\n",
    "correlation_minionkills = df['minionkills'].corr(df['result'])  \n",
    "print(\"Correlation with minionkills:\", correlation_minionkills)\n",
    "\n",
    "correlation_cspm = df['cspm'].corr(df['result'])  \n",
    "print(\"Correlation with cspm:\", correlation_cspm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unnecessary or Redundant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"atakhans\", \"opp_atakhans\"], inplace=True)\n",
    "\n",
    "columns_to_drop = (\n",
    "    df.columns[1:11]  # Metadata columns\n",
    "    .union(df.columns[12:18])  # Additional metadata columns\n",
    "    .union(df.columns[18:28])  # BP data\n",
    "    .union(df.columns[30:43])  # End game data columns\n",
    "    .union(df.columns[48:57])  # Drake-related columns\n",
    "    .union(df.columns[40:43])  # Individual data columns\n",
    "    .union(pd.Index([df.columns[78]]))  # Specific column (damageshare)\n",
    "    .union(pd.Index([df.columns[91]]))  # Specific column (earnedgoldshare)\n",
    "    .union(pd.Index([df.columns[95]]))  # Specific column (total cs)\n",
    "    .union(pd.Index([df.columns[28]]))  # Specific column (gamelength)\n",
    "    .union(df.columns[131:161])  # Data after 20 minutes\n",
    ")\n",
    "\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values in each column:\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_columns = null_counts[null_counts > 0]\n",
    "print(null_columns)\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "null_ratio = (null_counts / total_rows)\n",
    "\n",
    "null_columns_with_ratio = null_ratio[null_ratio > 0]\n",
    "print(\"Columns with null values and their ratios:\")\n",
    "print(null_columns_with_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop or fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['void_grubs'] = df['void_grubs'].fillna(0)\n",
    "df['opp_void_grubs'] = df['opp_void_grubs'].fillna(0)\n",
    "df['turretplates'] = df['turretplates'].fillna(0)\n",
    "df['opp_turretplates'] = df['opp_turretplates'].fillna(0)\n",
    "df['heralds'] = df['heralds'].fillna(0)\n",
    "df['opp_heralds'] = df['opp_heralds'].fillna(0)\n",
    "\n",
    "\n",
    "df['cspm'] = df['cspm'].fillna(df['cspm'].median())\n",
    "df['vspm'] = df['vspm'].fillna(df['vspm'].median())\n",
    "df['visionscore'] = df['visionscore'].fillna(df['visionscore'].median())\n",
    "df.drop(columns=['monsterkillsownjungle', 'monsterkillsenemyjungle'], inplace=True)\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop(columns=['gameid', 'side'], inplace=True)\n",
    "\n",
    "# # Print the count of null values in each column\n",
    "# print(\"Null values in each column:\")\n",
    "# null_counts = df.isnull().sum()\n",
    "# null_columns = null_counts[null_counts > 0]\n",
    "# print(null_columns)\n",
    "# print(\"----------------------------------------------------\")\n",
    "\n",
    "# # Calculate the ratio of null values for each column\n",
    "# null_ratio = (null_counts / total_rows)\n",
    "\n",
    "# # Filter and print only the columns where the ratio of null values is greater than 0\n",
    "# null_columns_with_ratio = null_ratio[null_ratio > 0]\n",
    "# print(\"Columns with null values and their ratios:\")\n",
    "# print(null_columns_with_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify That All Missing Values Are Handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the count of null values in each column\n",
    "print(\"Null values in each column:\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_columns = null_counts[null_counts > 0]\n",
    "print(null_columns)\n",
    "print(\"\")\n",
    "\n",
    "# Calculate the ratio of null values for each column\n",
    "null_ratio = (null_counts / total_rows)\n",
    "\n",
    "# Filter and print only the columns where the ratio of null values is greater than 0\n",
    "null_columns_with_ratio = null_ratio[null_ratio > 0]\n",
    "print(\"Columns with null values and their ratios:\")\n",
    "print(null_columns_with_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Variables into Numerical Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "champ_cols = ['top_champ', 'jng_champ', 'mid_champ', 'bot_champ', 'sup_champ']\n",
    "\n",
    "all_champs = pd.concat([df[col] for col in champ_cols], axis=0).unique()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(all_champs)\n",
    "\n",
    "for col in champ_cols:\n",
    "    df[col] = le.transform(df[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save champion-label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "champ_mapping = pd.DataFrame({\n",
    "    \"champion\": le.classes_,\n",
    "    \"label\": le.transform(le.classes_)\n",
    "})\n",
    "\n",
    "champ_mapping_path = \"data/champion_label_mapping.csv\"\n",
    "\n",
    "\n",
    "champ_mapping.to_csv(champ_mapping_path, index=False)\n",
    "print(f\"üìù Champion-label mapping saved to: {champ_mapping_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the head and tail of the dataframe\n",
    "head_and_tail = pd.concat([df.head(), df.tail()])\n",
    "\n",
    "display(pd.concat([df.head(), df.tail()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts = df['result'].value_counts()\n",
    "display(class_counts)\n",
    "\n",
    "df['result'].value_counts(normalize=True).plot(kind='bar', title='Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "print(\"Feature skewness: \")\n",
    "display(df.skew().sort_values(ascending=False))\n",
    "\n",
    "skewed = df.skew()[abs(df.skew()) > 1].index\n",
    "axes = df.hist(bins=30, figsize=(25, 20))\n",
    "\n",
    "# Annotate skewed features\n",
    "for ax in axes.flatten():\n",
    "    if ax.get_title() in skewed:\n",
    "        ax.set_title(ax.get_title(), color='red')\n",
    "\n",
    "plt.suptitle(\"Feature Distributions (Red = Skewed)\", fontsize=20)\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply normalization to skew data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Compute skewed features\n",
    "skewed_features = df.skew()[abs(df.skew()) >= 1].index.tolist()\n",
    "\n",
    "# Apply log transform to skewed + non-negative features\n",
    "for col in skewed_features:\n",
    "    if (df[col] >= 0).all():\n",
    "        df[col] = np.log1p(df[col])\n",
    "        df.rename(columns={col: f\"{col}_normalized\"}, inplace=True)\n",
    "    else:\n",
    "        print(f\"Feature {col} contains negative values, skip log transform\")\n",
    "\n",
    "# Define which columns to exclude from scaling\n",
    "label_col = 'result'\n",
    "binary_cols = [\n",
    "    'firstdragon', 'firstherald', 'firstbaron', 'firsttower',\n",
    "    'firstmidtower', 'firsttothreetowers'\n",
    "]\n",
    "categorical_cols = [\n",
    "    'top_champ', 'jng_champ', 'mid_champ', 'bot_champ', 'sup_champ'\n",
    "]\n",
    "exclude_cols = [label_col] + binary_cols + categorical_cols\n",
    "normalize_cols = df.columns.difference(exclude_cols)\n",
    "\n",
    "# Fit and apply MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[normalize_cols] = scaler.fit_transform(df[normalize_cols])\n",
    "\n",
    "# Save transformer and info\n",
    "joblib.dump(scaler, \"data/minmax_scaler.pkl\")\n",
    "with open(\"data/skewed_features.json\", \"w\") as f:\n",
    "    json.dump(skewed_features, f)\n",
    "with open(\"data/normalize_cols.json\", \"w\") as f:\n",
    "    json.dump(list(normalize_cols), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Distribution after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "\n",
    "axes = df.hist(bins=30, figsize=(25, 20))\n",
    "\n",
    "plt.suptitle(\"Feature Distributions after normalization\", fontsize=20)\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_means = df.groupby('result').mean()\n",
    "display(grouped_means)\n",
    "\n",
    "df.groupby('result').mean().T.plot(kind='bar', figsize=(25, 20), title='Feature Mean by Result')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Matrix / Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# correlation_matrix = df.corr()\n",
    "# display(correlation_matrix)\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', center=0, annot=False)\n",
    "plt.title('Correlation Heatmap', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the processed data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/processed_lol_data.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_numerical = [\n",
    "    True,  # team kpm\n",
    "    True,  # ckpm\n",
    "    False, # firstdragon\n",
    "    True,  # dragons\n",
    "    True,  # opp_dragons\n",
    "    True,  # elders_normalized\n",
    "    True,  # opp_elders_normalized\n",
    "    False, # firstherald\n",
    "    True,  # heralds\n",
    "    True,  # opp_heralds\n",
    "    True,  # void_grubs_normalized\n",
    "    True,  # opp_void_grubs_normalized\n",
    "    False, # firstbaron\n",
    "    True,  # barons\n",
    "    True,  # opp_barons\n",
    "    False, # firsttower\n",
    "    True,  # towers\n",
    "    True,  # opp_towers\n",
    "    False, # firstmidtower\n",
    "    False, # firsttothreetowers\n",
    "    True,  # turretplates\n",
    "    True,  # opp_turretplates\n",
    "    True,  # inhibitors_normalized\n",
    "    True,  # opp_inhibitors_normalized\n",
    "    True,  # damagetochampions\n",
    "    True,  # dpm\n",
    "    True,  # damagetakenperminute\n",
    "    True,  # damagemitigatedperminute_normalized\n",
    "    True,  # wardsplaced\n",
    "    True,  # wpm\n",
    "    True,  # wardskilled\n",
    "    True,  # wcpm\n",
    "    True,  # controlwardsbought\n",
    "    True,  # visionscore\n",
    "    True,  # vspm\n",
    "    True,  # totalgold\n",
    "    True,  # earnedgold\n",
    "    True,  # earned gpm\n",
    "    True,  # goldspent\n",
    "    True,  # gspd\n",
    "    True,  # gpr\n",
    "    True,  # minionkills\n",
    "    True,  # monsterkills\n",
    "    True,  # cspm\n",
    "    True,  # goldat10\n",
    "    True,  # xpat10\n",
    "    True,  # csat10\n",
    "    True,  # opp_goldat10\n",
    "    True,  # opp_xpat10\n",
    "    True,  # opp_csat10\n",
    "    True,  # golddiffat10\n",
    "    True,  # xpdiffat10\n",
    "    True,  # csdiffat10\n",
    "    True,  # killsat10_normalized\n",
    "    True,  # assistsat10_normalized\n",
    "    True,  # deathsat10_normalized\n",
    "    True,  # opp_killsat10_normalized\n",
    "    True,  # opp_assistsat10_normalized\n",
    "    True,  # opp_deathsat10_normalized\n",
    "    True,  # goldat15\n",
    "    True,  # xpat15\n",
    "    True,  # csat15\n",
    "    True,  # opp_goldat15\n",
    "    True,  # opp_xpat15\n",
    "    True,  # opp_csat15\n",
    "    True,  # golddiffat15\n",
    "    True,  # xpdiffat15\n",
    "    True,  # csdiffat15\n",
    "    True,  # killsat15_normalized\n",
    "    True,  # assistsat15_normalized\n",
    "    True,  # deathsat15_normalized\n",
    "    True,  # opp_killsat15_normalized\n",
    "    True,  # opp_assistsat15_normalized\n",
    "    True,  # opp_deathsat15_normalized\n",
    "    False, # top_champ\n",
    "    False, # jng_champ\n",
    "    False, # mid_champ\n",
    "    False, # bot_champ\n",
    "    False  # sup_champ\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, batch_size=64, test_size=0.2, seed=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torch\n",
    "\n",
    "    class LoLDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, df):\n",
    "            self.X = torch.tensor(df.drop(columns=['result']).values, dtype=torch.float32)\n",
    "            self.y = torch.tensor(df['result'].values, dtype=torch.long)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df['result'])\n",
    "\n",
    "    train_dataset = LoLDataset(train_df)\n",
    "    test_dataset = LoLDataset(test_df)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from lolnet import LoLNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from trades import trades_loss\n",
    "from jacobian import JacobianReg\n",
    "\n",
    "\n",
    "def add_input_noise(x, sigma=0.01, is_numerical=None):\n",
    "    noise = torch.randn_like(x) * sigma\n",
    "    if is_numerical is not None:\n",
    "        mask = torch.tensor(is_numerical, dtype=torch.float32, device=x.device)\n",
    "        noise *= mask\n",
    "    return torch.clamp(x + noise, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def generate_pgd_adversarial(model, x, y, epsilon=0.02, alpha=0.002, steps=10, is_numerical=None):\n",
    "    x_adv = x.clone().detach().requires_grad_(True)\n",
    "    mask = torch.tensor(is_numerical, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        output = model(x_adv)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, y)\n",
    "        grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "        x_adv = x_adv + alpha * grad.sign() * mask\n",
    "        x_adv = torch.min(torch.max(x_adv, x - epsilon), x + epsilon)\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0).detach().requires_grad_(True)\n",
    "\n",
    "    return x_adv.detach()\n",
    "\n",
    "\n",
    "def train_model_with_perturbation(\n",
    "    model, train_loader, test_loader, criterion, optimizer,\n",
    "    num_epochs=10, device='cpu', patience=3,\n",
    "    pgd=False, noise=False, trades=False, jacobian_reg=False,\n",
    "    is_numerical=None, pgd_config=None, noise_sigma=0.01, lambda_JR=0.01\n",
    "):\n",
    "    if pgd_config is None:\n",
    "        pgd_config = {'epsilon': 0.02, 'alpha': 0.002, 'steps': 10}\n",
    "\n",
    "    print(\"üîß Training Configuration:\")\n",
    "    print(f\"üìå Device       : {device}\")\n",
    "    print(f\"üìå Epochs       : {num_epochs}\")\n",
    "    print(f\"üìå Patience     : {patience}\")\n",
    "    print(f\"üìå PGD          : {pgd}\")\n",
    "    if pgd:\n",
    "        print(f\"   ‚Ü≥ epsilon    : {pgd_config['epsilon']}\")\n",
    "        print(f\"   ‚Ü≥ alpha      : {pgd_config['alpha']}\")\n",
    "        print(f\"   ‚Ü≥ steps      : {pgd_config['steps']}\")\n",
    "    print(f\"üìå Noise        : {noise}\")\n",
    "    if noise:\n",
    "        print(f\"   ‚Ü≥ sigma      : {noise_sigma}\")\n",
    "    print(f\"üìå TRADES       : {trades}\")\n",
    "    print(f\"üìå Jacobian Reg : {jacobian_reg}\")\n",
    "    print(f\"üìå Numerical Mask Present: {is_numerical is not None}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    best_acc, best_epoch = 0, -1\n",
    "    best_model_state = None\n",
    "    epoch_log = []\n",
    "    reg = JacobianReg(n=1) if jacobian_reg else None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            X_clean = X_batch\n",
    "            X_aug, y_aug = [], []\n",
    "\n",
    "            if pgd:\n",
    "                X_pgd = generate_pgd_adversarial(model, X_clean, y_batch, **pgd_config, is_numerical=is_numerical)\n",
    "                X_aug.append(X_pgd)\n",
    "                y_aug.append(y_batch)\n",
    "\n",
    "            if noise:\n",
    "                X_noisy = add_input_noise(X_clean, sigma=noise_sigma, is_numerical=is_numerical)\n",
    "                X_aug.append(X_noisy)\n",
    "                y_aug.append(y_batch)\n",
    "\n",
    "            if X_aug:\n",
    "                X_batch = torch.cat([X_clean] + X_aug, dim=0)\n",
    "                y_batch = torch.cat([y_batch] + y_aug, dim=0)\n",
    "\n",
    "            X_batch.requires_grad = True\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if trades:\n",
    "                loss = trades_loss(\n",
    "                    model=model,\n",
    "                    x_natural=X_clean,\n",
    "                    y=y_batch[:len(X_clean)],\n",
    "                    optimizer=optimizer,\n",
    "                    step_size=pgd_config['alpha'],\n",
    "                    epsilon=pgd_config['epsilon'],\n",
    "                    perturb_steps=pgd_config['steps'],\n",
    "                    distance='l_inf',\n",
    "                    is_numerical=is_numerical\n",
    "                )\n",
    "            else:\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "                if jacobian_reg and reg is not None:\n",
    "                    R = reg(X_batch, outputs)\n",
    "                    loss += lambda_JR * R\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = model(X_batch).argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        test_preds, test_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                preds = model(X_test).argmax(dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels.extend(y_test.cpu().numpy())\n",
    "\n",
    "        test_acc = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "        print(f\"üß™ Epoch {epoch+1:>2}/{num_epochs:<2} | \"\n",
    "              f\"Loss: {avg_loss:10.4f} | \"\n",
    "              f\"Train Acc: {train_acc:7.4f} | \"\n",
    "              f\"Test Acc: {test_acc:7.4f}\")\n",
    "\n",
    "        epoch_log.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'loss': avg_loss\n",
    "        })\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()\n",
    "        elif epoch - best_epoch >= patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, best_acc, epoch_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_lolnet_model(df, \n",
    "                       model_name, \n",
    "                       is_numerical, \n",
    "                       pgd=False,\n",
    "                       noise=False,\n",
    "                       trades=False,\n",
    "                       jacobian_reg=False\n",
    "                       ):\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    num_epochs = 30\n",
    "    patience = 10\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "\n",
    "    train_loader, test_loader = create_dataloaders(df, batch_size=batch_size)\n",
    "    input_dim = df.drop(columns=['result']).shape[1]\n",
    "\n",
    "    model = LoLNet(input_dim=input_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    model, best_acc, epoch_log = train_model_with_perturbation(\n",
    "        model, train_loader, test_loader,\n",
    "        criterion, optimizer,\n",
    "        num_epochs=num_epochs, \n",
    "        device=device,\n",
    "        patience=patience,\n",
    "        pgd=pgd,\n",
    "        noise=noise,\n",
    "        trades=trades,\n",
    "        jacobian_reg=jacobian_reg,\n",
    "        is_numerical=is_numerical,\n",
    "        pgd_config={ 'epsilon': 0.02,'alpha': 0.002,'steps': 10 },\n",
    "        noise_sigma=0.01\n",
    "    )\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    model_path = f\"models/{model_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"‚úÖ Training complete. Best test accuracy: {best_acc:.4f}\")\n",
    "    print(f\"üíæ Model saved to: {model_path}\")\n",
    "\n",
    "    dummy_input = torch.randn(1, input_dim).to(device)\n",
    "    onnx_path = f\"models/{model_name}.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input,),\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "        opset_version=11\n",
    "    )\n",
    "    print(f\"üß† ONNX model exported to: {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lolnet_model(df, \"lolnet\", is_numerical, )\n",
    "train_lolnet_model(df, \"lolnet_pgd\", is_numerical, pgd=True)\n",
    "train_lolnet_model(df, \"lolnet_noise\", is_numerical, noise=True)\n",
    "train_lolnet_model(df, \"lolnet_trades\", is_numerical, trades=True)\n",
    "train_lolnet_model(df, \"lolnet_jacobian\", is_numerical, jacobian_reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_numerical = [\n",
    "    True,  # team kpm\n",
    "    True,  # ckpm\n",
    "    False, # firstdragon\n",
    "    True,  # dragons\n",
    "    True,  # opp_dragons\n",
    "    True,  # elders_normalized\n",
    "    True,  # opp_elders_normalized\n",
    "    False, # firstherald\n",
    "    True,  # heralds\n",
    "    True,  # opp_heralds\n",
    "    True,  # void_grubs_normalized\n",
    "    True,  # opp_void_grubs_normalized\n",
    "    False, # firstbaron\n",
    "    True,  # barons\n",
    "    True,  # opp_barons\n",
    "    False, # firsttower\n",
    "    True,  # towers\n",
    "    True,  # opp_towers\n",
    "    False, # firstmidtower\n",
    "    False, # firsttothreetowers\n",
    "    True,  # turretplates\n",
    "    True,  # opp_turretplates\n",
    "    True,  # inhibitors_normalized\n",
    "    True,  # opp_inhibitors_normalized\n",
    "    True,  # damagetochampions\n",
    "    True,  # dpm\n",
    "    True,  # damagetakenperminute\n",
    "    True,  # damagemitigatedperminute_normalized\n",
    "    True,  # wardsplaced\n",
    "    True,  # wpm\n",
    "    True,  # wardskilled\n",
    "    True,  # wcpm\n",
    "    True,  # controlwardsbought\n",
    "    True,  # visionscore\n",
    "    True,  # vspm\n",
    "    True,  # totalgold\n",
    "    True,  # earnedgold\n",
    "    True,  # earned gpm\n",
    "    True,  # goldspent\n",
    "    True,  # gspd\n",
    "    True,  # gpr\n",
    "    True,  # minionkills\n",
    "    True,  # monsterkills\n",
    "    True,  # cspm\n",
    "    True,  # goldat10\n",
    "    True,  # xpat10\n",
    "    True,  # csat10\n",
    "    True,  # opp_goldat10\n",
    "    True,  # opp_xpat10\n",
    "    True,  # opp_csat10\n",
    "    True,  # golddiffat10\n",
    "    True,  # xpdiffat10\n",
    "    True,  # csdiffat10\n",
    "    True,  # killsat10_normalized\n",
    "    True,  # assistsat10_normalized\n",
    "    True,  # deathsat10_normalized\n",
    "    True,  # opp_killsat10_normalized\n",
    "    True,  # opp_assistsat10_normalized\n",
    "    True,  # opp_deathsat10_normalized\n",
    "    True,  # goldat15\n",
    "    True,  # xpat15\n",
    "    True,  # csat15\n",
    "    True,  # opp_goldat15\n",
    "    True,  # opp_xpat15\n",
    "    True,  # opp_csat15\n",
    "    True,  # golddiffat15\n",
    "    True,  # xpdiffat15\n",
    "    True,  # csdiffat15\n",
    "    True,  # killsat15_normalized\n",
    "    True,  # assistsat15_normalized\n",
    "    True,  # deathsat15_normalized\n",
    "    True,  # opp_killsat15_normalized\n",
    "    True,  # opp_assistsat15_normalized\n",
    "    True,  # opp_deathsat15_normalized\n",
    "    False, # top_champ\n",
    "    False, # jng_champ\n",
    "    False, # mid_champ\n",
    "    False, # bot_champ\n",
    "    False  # sup_champ\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Verified Robustness Rate Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maraboupy import Marabou\n",
    "from maraboupy.MarabouNetworkONNX import MarabouNetworkONNX\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import sys\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    " \n",
    "\n",
    "def test_lolnet_verified_robustness_rate(\n",
    "    df: pd.DataFrame,\n",
    "    onnx_path: str,\n",
    "    epsilon: float,\n",
    "    is_numerical:list[bool],\n",
    "    num_samples: int = 100,\n",
    "    verbosity: int = 1\n",
    "):\n",
    "    from maraboupy import MarabouNetworkONNX \n",
    "    options = Marabou.createOptions(verbosity=0)\n",
    "    network:MarabouNetworkONNX  = Marabou.read_onnx(onnx_path)\n",
    "\n",
    "    inputVars = network.inputVars[0][0]\n",
    "    outputVars = network.outputVars[0][0]\n",
    "\n",
    "    \n",
    "    if num_samples == -1:\n",
    "        samples = df.iterrows()\n",
    "    else:\n",
    "        assert num_samples <= len(df), \"Number of samples exceeds the size of the dataframe.\"\n",
    "        samples = df.sample(n=num_samples, random_state=42).iterrows()\n",
    "    \n",
    "    input_dim = len(inputVars)\n",
    "    results = []\n",
    "    \n",
    "    samples = list(samples)\n",
    "\n",
    "    for idx, row in tqdm(samples, desc=\"Verifying\", unit=\"sample\", leave=True):\n",
    "        x0 = row[1:].values.tolist()\n",
    "        true_label = int(row.iloc[0])\n",
    "\n",
    "        assert len(x0) == input_dim, \"Input dimension mismatch.\"\n",
    "\n",
    "        for i, x_i in enumerate(x0):\n",
    "            eps_i = epsilon if is_numerical[i] else 0.0\n",
    "            network.setLowerBound(inputVars[i], x_i - eps_i)\n",
    "            network.setUpperBound(inputVars[i], x_i + eps_i)\n",
    "\n",
    "\n",
    "        y0, y1 = outputVars[0], outputVars[1]\n",
    "        if true_label == 0:\n",
    "            network.addInequality([y1, y0], [1, -1], 0) \n",
    "        else:\n",
    "            network.addInequality([y0, y1], [1, -1], 0) \n",
    "\n",
    "        # Solve\n",
    "        # with suppress_stdout():\n",
    "        #     status, assignments, stats = network.solve(options=options)\n",
    "        # status, assignments, stats = network.solve(options=options)\n",
    "        \n",
    "        orig_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        try:\n",
    "            status, assignments, stats = network.solve(options=options)\n",
    "        finally:\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = orig_stdout\n",
    "\n",
    "\n",
    "        results.append((idx, true_label, status))\n",
    "        \n",
    "        if verbosity == 1:\n",
    "            if status == \"sat\":\n",
    "                print(f\"‚ö†Ô∏è SAT ‚Äì idx {idx} | label: {true_label}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ UNSAT ‚Äì idx {idx} | label: {true_label}\")\n",
    "        elif verbosity == 2:\n",
    "            if status == \"sat\":\n",
    "                print(f\"‚ö†Ô∏è SAT ‚Äì idx {idx} | label: {true_label}\")\n",
    "                for i, var in enumerate(inputVars):\n",
    "                    val = assignments.get(var, None)\n",
    "                    try:\n",
    "                        print(f\"  x{i}: {float(val):.5f}\")\n",
    "                    except:\n",
    "                        print(f\"  x{i}: {val}\")\n",
    "\n",
    "                for j, out_var in enumerate(outputVars):\n",
    "                    val = assignments.get(out_var, None)\n",
    "                    try:\n",
    "                        print(f\"  y{j}: {float(val):.5f}\")\n",
    "                    except:\n",
    "                        print(f\"  y{j}: {val}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ UNSAT ‚Äì idx {idx} | label: {true_label}\")\n",
    "\n",
    "\n",
    "    num_total = len(results)\n",
    "    num_sat = sum(1 for r in results if r[2] == \"sat\")\n",
    "    num_unsat = num_total - num_sat\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Total samples tested: {num_total}\")\n",
    "    print(f\"  SAT (adversarial found): {num_sat}\")\n",
    "    print(f\"  UNSAT (robust): {num_unsat}\")\n",
    "    \n",
    "    return {\n",
    "        \"total_samples\": num_total,\n",
    "        \"num_verified_robust\": num_unsat,\n",
    "        \"num_adversarial_found\": num_sat,\n",
    "        \"verified_robustness_rate\": num_unsat / num_total,\n",
    "        \"results\": results \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Robustness Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def test_lolnet_robustness_accuracy(\n",
    "    model,\n",
    "    df,\n",
    "    is_numerical,\n",
    "    epsilon_val=0.01,\n",
    "    batch_size=64,\n",
    "    alpha=0.005,\n",
    "    steps=20,\n",
    "    device='cpu'\n",
    "):\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    X = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)\n",
    "    y = torch.tensor(df.iloc[:, 0].values, dtype=torch.long)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    epsilon = torch.tensor(\n",
    "        [epsilon_val if is_numerical[i] else 0.0 for i in range(input_dim)],\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    mask = torch.tensor(is_numerical, dtype=torch.float32, device=device)\n",
    "\n",
    "    def masked_pgd(x, y):\n",
    "        x_orig = x.detach()\n",
    "        x_adv = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            outputs = model(x_adv)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, y)\n",
    "\n",
    "            grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "            grad = grad.sign() * mask \n",
    "            update = alpha * grad\n",
    "\n",
    "            x_adv = x_adv + update\n",
    "            x_adv = torch.min(torch.max(x_adv, x_orig - epsilon), x_orig + epsilon)\n",
    "            x_adv = torch.clamp(x_adv, 0.0, 1.0).detach().requires_grad_(True)\n",
    "\n",
    "        return x_adv.detach()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x_batch = X[i:i+batch_size].to(device)\n",
    "        y_batch = y[i:i+batch_size].to(device)\n",
    "\n",
    "        adv_x = masked_pgd(x_batch, y_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(adv_x).argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    robust_accuracy = correct / total\n",
    "    print(f\"üîê Robustness Accuracy: {robust_accuracy:.2%}\")\n",
    "    return robust_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Gradient Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def get_gradient_norms(model, df: pd.DataFrame, is_numerical: list[bool], norm_type=2, batch_size=64):\n",
    "    device = next(model.parameters()).device \n",
    "\n",
    "    X_test = df.drop(columns=[\"result\"]).values\n",
    "    mask = torch.tensor(is_numerical, dtype=torch.float32, device=device)\n",
    "    X_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "\n",
    "    def masked_gradient_norm(x):\n",
    "        x = x.clone().detach().requires_grad_(True)\n",
    "        outputs = model(x)\n",
    "        scores = outputs.max(1)[0]\n",
    "        grads = torch.autograd.grad(scores, x,\n",
    "                                    grad_outputs=torch.ones_like(scores),\n",
    "                                    retain_graph=False, create_graph=False)[0]\n",
    "        masked_grads = grads * mask\n",
    "        if norm_type == 'inf':\n",
    "            return masked_grads.abs().max(dim=1)[0]\n",
    "        return masked_grads.norm(p=norm_type, dim=1)\n",
    "\n",
    "    model.eval()\n",
    "    all_norms = []\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        x_batch = X_tensor[i:i + batch_size]\n",
    "        norms = masked_gradient_norm(x_batch)\n",
    "        all_norms.append(norms)\n",
    "\n",
    "    all_norms_tensor = torch.cat(all_norms)\n",
    "    print(f\"‚úÖ Gradient Norms ‚Äî Mean: {all_norms_tensor.mean():.4f} | Min: {all_norms_tensor.min():.4f} | Max: {all_norms_tensor.max():.4f}\")\n",
    "    return all_norms_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Clean Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def test_lolnet_clean_accuracy(\n",
    "    model,\n",
    "    df,\n",
    "    batch_size=64,\n",
    "    device='cpu'\n",
    "):\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    X = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)\n",
    "    y = torch.tensor(df.iloc[:, 0].values, dtype=torch.long)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x_batch = X[i:i+batch_size].to(device)\n",
    "        y_batch = y[i:i+batch_size].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(x_batch).argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"‚úÖ Clean Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test verified robustness rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")\n",
    "_, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['result'])\n",
    "\n",
    "onnx_files = sorted(glob.glob(\"models/*.onnx\"))\n",
    "\n",
    "for onnx_path in onnx_files:\n",
    "    model_name = os.path.splitext(os.path.basename(onnx_path))[0]\n",
    "    print(f\"\\nüß™ Verifying: {model_name}\")\n",
    "    for epsilon in [0.01]:\n",
    "        print(f\"{epsilon = }\")\n",
    "        test_lolnet_verified_robustness_rate(\n",
    "            test_df,\n",
    "            onnx_path=onnx_path,\n",
    "            epsilon=0.05,\n",
    "            num_samples=1000,\n",
    "            is_numerical=is_numerical,\n",
    "            verbosity=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Robustness Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lolnet import LoLNet\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")\n",
    "_, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['result'])\n",
    "\n",
    "input_dim = df.drop(columns=[\"result\"]).shape[1]\n",
    "device = \"cpu\"\n",
    "print(f\"{device = }\")\n",
    "\n",
    "pth_files = sorted(glob.glob(\"models/*.pth\"))\n",
    "\n",
    "for model_path in pth_files:\n",
    "    model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "    print(f\"\\nüß™ Verifying: {model_name}\")\n",
    "\n",
    "    model = LoLNet(input_dim=input_dim, hidden_dim=64).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    for epsilon in [0.01, 0.03, 0.05, 0.07, 0.09]:\n",
    "        print(f\"{epsilon = }\")\n",
    "        robust_accuracy = test_lolnet_robustness_accuracy(\n",
    "            model=model,\n",
    "            df=test_df,\n",
    "            is_numerical=is_numerical,\n",
    "            epsilon_val=epsilon,\n",
    "            device=device\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gradient Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from lolnet import LoLNet\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")\n",
    "_, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['result'])\n",
    "\n",
    "input_dim = df.drop(columns=[\"result\"]).shape[1]\n",
    "device = \"cpu\"\n",
    "print(f\"{device = }\")\n",
    "\n",
    "model_files = sorted(glob.glob(\"models/*.pth\"))\n",
    "\n",
    "for model_path in model_files:\n",
    "    model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "    print(f\"\\nüß™ Verifying: {model_name}\")\n",
    "\n",
    "    model = LoLNet(input_dim=input_dim, hidden_dim=64) \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    gradient_norm = get_gradient_norms(\n",
    "        model=model,\n",
    "        df=test_df,\n",
    "        is_numerical=is_numerical\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy on Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from lolnet import LoLNet\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")\n",
    "_, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['result'])\n",
    "\n",
    "input_dim = df.drop(columns=[\"result\"]).shape[1]\n",
    "device = \"cpu\"\n",
    "print(f\"{device = }\")\n",
    "\n",
    "model_files = sorted(glob.glob(\"models/*.pth\"))\n",
    "\n",
    "for model_path in model_files:\n",
    "    model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "    print(f\"\\nüß™ Verifying: {model_name}\")\n",
    "\n",
    "    model = LoLNet(input_dim=input_dim, hidden_dim=64).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    clean_accuracy = test_lolnet_clean_accuracy(\n",
    "        model=model,\n",
    "        df=test_df,\n",
    "        device=device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def spectral_norm(layer, n_iter=10):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        W = layer.weight.data\n",
    "        device = W.device\n",
    "        u = torch.randn(W.size(0), 1, device=device)  \n",
    "        for _ in range(n_iter):\n",
    "            v = F.normalize(torch.matmul(W.t(), u), dim=0)\n",
    "            u = F.normalize(torch.matmul(W, v), dim=0)\n",
    "        sigma = torch.dot(u.squeeze(), torch.matmul(W, v).squeeze())\n",
    "        return sigma.item()\n",
    "    else:\n",
    "        return 1.0  \n",
    "\n",
    "def compute_lipschitz_constant(model):\n",
    "    lipschitz = 1.0\n",
    "    for layer in model.model:\n",
    "        lipschitz *= spectral_norm(layer)\n",
    "    print(f\"\\n‚úÖ Estimated Global Lipschitz Constant: {lipschitz:.4f}\")\n",
    "    return lipschitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from lolnet import LoLNet\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")\n",
    "_, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['result'])\n",
    "\n",
    "input_dim = df.drop(columns=[\"result\"]).shape[1]\n",
    "device = \"cpu\"\n",
    "print(f\"{device = }\")\n",
    "\n",
    "model_files = sorted(glob.glob(\"models/*.pth\"))\n",
    "\n",
    "for model_path in model_files:\n",
    "    model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "    print(f\"\\nüß™ Verifying: {model_name}\")\n",
    "\n",
    "    model = LoLNet(input_dim=input_dim, hidden_dim=64).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "\n",
    "    lips_score = compute_lipschitz_constant(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
