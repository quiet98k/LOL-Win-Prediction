{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Preprocessing\n",
    "## Data Preprocessing\n",
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            f\"data/OraclesElixir/{year}_LoL_esports_match_data_from_OraclesElixir.csv\",\n",
    "            dtype={\"url\": \"str\"}\n",
    "        )\n",
    "        for year in range(2020, 2025)\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "rows, cols = df.shape\n",
    "print(f\"The CSV file has {rows} rows and {cols} columns.\")\n",
    "\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for Complete Matches Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_complete_rows = df[df[\"datacompleteness\"] == 'complete'].shape[0]\n",
    "total_rows = df.shape[0]\n",
    "ratio = num_complete_rows / total_rows\n",
    "print(f\"Number of rows where datacompleteness is 'complete': {num_complete_rows}\")\n",
    "print(f\"Ratio of 'complete' rows to total rows: {ratio:.4f}\")\n",
    "\n",
    "df = df[df[\"datacompleteness\"] == 'complete']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Individual Stats to Team-Level Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_rows = df[df['position'] == 'team'].copy()\n",
    "player_rows = df[df['position'] != 'team']\n",
    "\n",
    "positions = ['top', 'jng', 'mid', 'bot', 'sup']\n",
    "\n",
    "for pos in positions:\n",
    "    champ_col = (\n",
    "        player_rows[player_rows['position'] == pos]\n",
    "        .loc[:, ['gameid', 'side', 'champion']]\n",
    "        .rename(columns={'champion': f'{pos}_champ'})\n",
    "    )\n",
    "    \n",
    "    team_rows = team_rows.merge(champ_col, on=['gameid', 'side'], how='left')\n",
    "df = team_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_void = df['void_grubs'].corr(df['result'])  \n",
    "print(\"Correlation with void:\", correlation_void)\n",
    "\n",
    "correlation_monsterkillsownjungle = df['monsterkillsownjungle'].corr(df['result'])  \n",
    "print(\"Correlation with monsterkillsownjungle:\", correlation_monsterkillsownjungle)\n",
    "\n",
    "correlation_turretplates = df['turretplates'].corr(df['result'])  \n",
    "print(\"Correlation with turretplates:\", correlation_turretplates)\n",
    "\n",
    "correlation_heralds = df['heralds'].corr(df['result'])  \n",
    "print(\"Correlation with heralds:\", correlation_heralds)\n",
    "\n",
    "correlation_visionscore = df['visionscore'].corr(df['result'])  \n",
    "print(\"Correlation with visionscore:\", correlation_visionscore)\n",
    "\n",
    "correlation_vspm = df['vspm'].corr(df['result'])  \n",
    "print(\"Correlation with vspm:\", correlation_vspm)\n",
    "\n",
    "correlation_minionkills = df['minionkills'].corr(df['result'])  \n",
    "print(\"Correlation with minionkills:\", correlation_minionkills)\n",
    "\n",
    "correlation_cspm = df['cspm'].corr(df['result'])  \n",
    "print(\"Correlation with cspm:\", correlation_cspm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unnecessary or Redundant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = (\n",
    "    df.columns[1:11]  # Metadata columns\n",
    "    .union(df.columns[12:18])  # Additional metadata columns\n",
    "    .union(df.columns[18:28])  # BP data\n",
    "    .union(df.columns[30:43])  # End game data columns\n",
    "    .union(df.columns[48:57])  # Drake-related columns\n",
    "    .union(df.columns[40:43])  # Individual data columns\n",
    "    .union(pd.Index([df.columns[78]]))  # Specific column (damageshare)\n",
    "    .union(pd.Index([df.columns[91]]))  # Specific column (earnedgoldshare)\n",
    "    .union(pd.Index([df.columns[95]]))  # Specific column (total cs)\n",
    "    .union(pd.Index([df.columns[28]]))  # Specific column (gamelength)\n",
    "    .union(df.columns[131:161])  # Data after 20 minutes\n",
    ")\n",
    "\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the count of null values in each column\n",
    "print(\"Null values in each column:\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_columns = null_counts[null_counts > 0]\n",
    "print(null_columns)\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "# Calculate the ratio of null values for each column\n",
    "null_ratio = (null_counts / total_rows)\n",
    "\n",
    "# Filter and print only the columns where the ratio of null values is greater than 0\n",
    "null_columns_with_ratio = null_ratio[null_ratio > 0]\n",
    "print(\"Columns with null values and their ratios:\")\n",
    "print(null_columns_with_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop or fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['void_grubs'] = df['void_grubs'].fillna(0)\n",
    "df['opp_void_grubs'] = df['opp_void_grubs'].fillna(0)\n",
    "df['turretplates'] = df['turretplates'].fillna(0)\n",
    "df['opp_turretplates'] = df['opp_turretplates'].fillna(0)\n",
    "df['heralds'] = df['heralds'].fillna(0)\n",
    "df['opp_heralds'] = df['opp_heralds'].fillna(0)\n",
    "\n",
    "\n",
    "df['cspm'] = df['cspm'].fillna(df['cspm'].median())\n",
    "df['vspm'] = df['vspm'].fillna(df['vspm'].median())\n",
    "df['visionscore'] = df['visionscore'].fillna(df['visionscore'].median())\n",
    "df.drop(columns=['monsterkillsownjungle', 'monsterkillsenemyjungle'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.drop(columns=['gameid', 'side'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify That All Missing Values Are Handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the count of null values in each column\n",
    "print(\"Null values in each column:\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_columns = null_counts[null_counts > 0]\n",
    "print(null_columns)\n",
    "print(\"\")\n",
    "\n",
    "# Calculate the ratio of null values for each column\n",
    "null_ratio = (null_counts / total_rows)\n",
    "\n",
    "# Filter and print only the columns where the ratio of null values is greater than 0\n",
    "null_columns_with_ratio = null_ratio[null_ratio > 0]\n",
    "print(\"Columns with null values and their ratios:\")\n",
    "print(null_columns_with_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Variables into Numerical Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "champ_cols = ['top_champ', 'jng_champ', 'mid_champ', 'bot_champ', 'sup_champ']\n",
    "\n",
    "all_champs = pd.concat([df[col] for col in champ_cols], axis=0).unique()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(all_champs)\n",
    "\n",
    "for col in champ_cols:\n",
    "    df[col] = le.transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the head and tail of the dataframe\n",
    "head_and_tail = pd.concat([df.head(), df.tail()])\n",
    "\n",
    "# Save to a CSV file\n",
    "head_and_tail.to_csv(\"head_and_tail.csv\", index=False)\n",
    "display(pd.concat([df.head(), df.tail()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts = df['result'].value_counts()\n",
    "display(class_counts)\n",
    "\n",
    "df['result'].value_counts(normalize=True).plot(kind='bar', title='Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "print(\"Feature skewness: \")\n",
    "display(df.skew().sort_values(ascending=False))\n",
    "\n",
    "skewed = df.skew()[abs(df.skew()) > 1].index\n",
    "axes = df.hist(bins=30, figsize=(25, 20))\n",
    "\n",
    "# Annotate skewed features\n",
    "for ax in axes.flatten():\n",
    "    if ax.get_title() in skewed:\n",
    "        ax.set_title(ax.get_title(), color='red')\n",
    "\n",
    "plt.suptitle(\"Feature Distributions (Red = Skewed)\", fontsize=20)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply normalization to skew data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "skewed_features = df.skew()[abs(df.skew()) >= 1].index\n",
    "\n",
    "for col in skewed_features:\n",
    "    if (df[col] >= 0).all(): \n",
    "        df[col] = np.log1p(df[col])\n",
    "    else:\n",
    "        print(f\"Feture {col} contains negative values, abort normalization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "label_col = 'result'\n",
    "binary_cols = [\n",
    "    'firstdragon', 'firstherald', 'firstbaron', 'firsttower',\n",
    "    'firstmidtower', 'firsttothreetowers'\n",
    "]\n",
    "\n",
    "exclude_cols = [label_col] + binary_cols\n",
    "\n",
    "normalize_cols = df.columns.difference(exclude_cols)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[normalize_cols] = scaler.fit_transform(df[normalize_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Distribution after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "\n",
    "axes = df.hist(bins=30, figsize=(25, 20))\n",
    "\n",
    "plt.suptitle(\"Feature Distributions (Red = Skewed)\", fontsize=20)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_means = df.groupby('result').mean()\n",
    "display(grouped_means)\n",
    "\n",
    "df.groupby('result').mean().T.plot(kind='bar', figsize=(25, 20), title='Feature Mean by Result')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Matrix / Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# correlation_matrix = df.corr()\n",
    "# display(correlation_matrix)\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', center=0, annot=False)\n",
    "plt.title('Correlation Heatmap', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the processed data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/processed_lol_data.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, batch_size=64, test_size=0.2, seed=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torch\n",
    "\n",
    "    class LoLDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, df):\n",
    "            self.X = torch.tensor(df.drop(columns=['result']).values, dtype=torch.float32)\n",
    "            self.y = torch.tensor(df['result'].values, dtype=torch.long)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df['result'])\n",
    "\n",
    "    train_dataset = LoLDataset(train_df)\n",
    "    test_dataset = LoLDataset(test_df)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LoLNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        super(LoLNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10, device='cpu', trial=None, patience=3):\n",
    "    model.to(device)\n",
    "    best_acc = 0\n",
    "    epoch_log = []\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        model.eval()\n",
    "        test_preds, test_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        test_acc = accuracy_score(test_labels, test_preds)\n",
    "        print(f\"🧪 Epoch {epoch+1:>2}/{num_epochs:<2} | \"\n",
    "            f\"Loss: {total_loss:10.4f} | \"\n",
    "            f\"Train Acc: {train_acc:7.4f} | \"\n",
    "            f\"Test Acc: {test_acc:7.4f}\")\n",
    "\n",
    "\n",
    "        \n",
    "        epoch_log.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'loss': total_loss\n",
    "        })\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()\n",
    "        elif epoch - best_epoch >= patience:\n",
    "            print(f\"⏹️ Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "        # if trial is not None:\n",
    "        #     trial.report(test_acc, step=epoch)\n",
    "        #     if trial.should_prune():\n",
    "        #         print(f\"🔪 Trial pruned at epoch {epoch+1}\")\n",
    "        #         raise optuna.TrialPruned()\n",
    "\n",
    "        best_acc = max(best_acc, test_acc)\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "    return best_acc, epoch_log\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameter tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    # patience = trial.suggest_int(\"patience\", 2, 6)\n",
    "    patience = 6\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 10, 50)\n",
    "    # num_epochs = 30 \n",
    "    \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader, test_loader = create_dataloaders(df, batch_size=batch_size)\n",
    "    input_dim = df.drop(columns=['result']).shape[1]\n",
    "\n",
    "    model = LoLNet(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    acc, epoch_log = train_model(\n",
    "        model, train_loader, test_loader, \n",
    "        criterion, optimizer, \n",
    "        num_epochs=num_epochs, device=device,\n",
    "        trial=trial, patience=patience\n",
    "    )\n",
    "    \n",
    "    trial.set_user_attr(\"epoch_log\", epoch_log)\n",
    "\n",
    "    model_path = f\"models/trials/model_trial_{trial.number}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    trial.set_user_attr(\"saved_model_path\", model_path)\n",
    "    print(f\"💾 Model for Trial {trial.number} saved to: {model_path}\")\n",
    "\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "import shutil\n",
    "\n",
    "def callback(study, trial):\n",
    "    best = study.best_trial\n",
    "    print(f\"✅ Trial {trial.number} | Accuracy: {trial.value:.4f} | Params: {trial.params} | \"\n",
    "          f\"🏆 Best: Trial {best.number} ({best.value:.4f}) \\n\")\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, callbacks=[callback])\n",
    "\n",
    "\n",
    "best_model_path_before = study.best_trial.user_attrs[\"saved_model_path\"]\n",
    "best_model_path_after = f\"models/best_model.pth\"\n",
    "shutil.copy(best_model_path_before, best_model_path_after)\n",
    "print(f\"✅ Best model copied to: {best_model_path_after}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trial_data = []\n",
    "\n",
    "for trial in study.trials:\n",
    "    if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "        row = {\n",
    "            \"trial\": trial.number,\n",
    "            \"accuracy\": trial.value,\n",
    "            **trial.params,  \n",
    "        }\n",
    "\n",
    "        if \"epoch_log\" in trial.user_attrs:\n",
    "            row[\"actual_epochs\"] = len(trial.user_attrs[\"epoch_log\"])\n",
    "\n",
    "        if \"saved_model_path\" in trial.user_attrs:\n",
    "            row[\"model_path\"] = trial.user_attrs[\"saved_model_path\"]\n",
    "\n",
    "        trial_data.append(row)\n",
    "\n",
    "df_trials = pd.DataFrame(trial_data)\n",
    "df_trials.to_csv(\"training_log/trial_results.csv\", index=False)\n",
    "print(\"✅ Saved trial results to trial_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_logs = []\n",
    "\n",
    "for trial in study.trials:\n",
    "    if trial.state == optuna.trial.TrialState.COMPLETE and \"epoch_log\" in trial.user_attrs:\n",
    "        for e in trial.user_attrs[\"epoch_log\"]:\n",
    "            log_row = {\n",
    "                \"trial\": trial.number,\n",
    "                **trial.params,\n",
    "                **e  \n",
    "            }\n",
    "            epoch_logs.append(log_row)\n",
    "\n",
    "df_epochs = pd.DataFrame(epoch_logs)\n",
    "df_epochs.to_csv(\"training_log/epoch_logs.csv\", index=False)\n",
    "print(\"✅ Saved per-epoch logs to epoch_logs.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"🏆 Best Trial: {best_trial.number}\")\n",
    "print(f\"✅ Accuracy: {best_trial.value:.4f}\")\n",
    "print(f\"📦 Hyperparameters: {best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get best trial and epoch log\n",
    "best_trial = study.best_trial\n",
    "epoch_log = best_trial.user_attrs[\"epoch_log\"]\n",
    "\n",
    "# Extract values\n",
    "epochs     = [e['epoch'] for e in epoch_log]\n",
    "train_accs = [e['train_acc'] for e in epoch_log]\n",
    "test_accs  = [e['test_acc'] for e in epoch_log]\n",
    "losses     = [e['loss'] for e in epoch_log]\n",
    "\n",
    "# Create two side-by-side subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(epochs, train_accs, label=\"Train Accuracy\", marker='o')\n",
    "ax1.plot(epochs, test_accs, label=\"Test Accuracy\", marker='o')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_title(f\"Best Trial {best_trial.number} - Accuracy\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(epochs, losses, label=\"Loss\", color='gray', linestyle='--', marker='x')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Loss\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for local robustness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
