{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Preprocessing\n",
    "## Data Preprocessing\n",
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            f\"data/OraclesElixir/{year}_LoL_esports_match_data_from_OraclesElixir.csv\",\n",
    "            dtype={\"url\": \"str\"}\n",
    "        )\n",
    "        for year in range(2020, 2025)\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "rows, cols = df.shape\n",
    "print(f\"The CSV file has {rows} rows and {cols} columns.\")\n",
    "\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for Complete Matches Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_complete_rows = df[df[\"datacompleteness\"] == 'complete'].shape[0]\n",
    "total_rows = df.shape[0]\n",
    "ratio = num_complete_rows / total_rows\n",
    "print(f\"Number of rows where datacompleteness is 'complete': {num_complete_rows}\")\n",
    "print(f\"Ratio of 'complete' rows to total rows: {ratio:.4f}\")\n",
    "\n",
    "df = df[df[\"datacompleteness\"] == 'complete']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Individual Stats to Team-Level Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_rows = df[df['position'] == 'team'].copy()\n",
    "player_rows = df[df['position'] != 'team']\n",
    "\n",
    "positions = ['top', 'jng', 'mid', 'bot', 'sup']\n",
    "\n",
    "for pos in positions:\n",
    "    champ_col = (\n",
    "        player_rows[player_rows['position'] == pos]\n",
    "        .loc[:, ['gameid', 'side', 'champion']]\n",
    "        .rename(columns={'champion': f'{pos}_champ'})\n",
    "    )\n",
    "    \n",
    "    team_rows = team_rows.merge(champ_col, on=['gameid', 'side'], how='left')\n",
    "df = team_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_void = df['void_grubs'].corr(df['result'])  \n",
    "print(\"Correlation with void:\", correlation_void)\n",
    "\n",
    "correlation_monsterkillsownjungle = df['monsterkillsownjungle'].corr(df['result'])  \n",
    "print(\"Correlation with monsterkillsownjungle:\", correlation_monsterkillsownjungle)\n",
    "\n",
    "correlation_turretplates = df['turretplates'].corr(df['result'])  \n",
    "print(\"Correlation with turretplates:\", correlation_turretplates)\n",
    "\n",
    "correlation_heralds = df['heralds'].corr(df['result'])  \n",
    "print(\"Correlation with heralds:\", correlation_heralds)\n",
    "\n",
    "correlation_visionscore = df['visionscore'].corr(df['result'])  \n",
    "print(\"Correlation with visionscore:\", correlation_visionscore)\n",
    "\n",
    "correlation_vspm = df['vspm'].corr(df['result'])  \n",
    "print(\"Correlation with vspm:\", correlation_vspm)\n",
    "\n",
    "correlation_minionkills = df['minionkills'].corr(df['result'])  \n",
    "print(\"Correlation with minionkills:\", correlation_minionkills)\n",
    "\n",
    "correlation_cspm = df['cspm'].corr(df['result'])  \n",
    "print(\"Correlation with cspm:\", correlation_cspm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unnecessary or Redundant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = (\n",
    "    df.columns[1:11]  # Metadata columns\n",
    "    .union(df.columns[12:18])  # Additional metadata columns\n",
    "    .union(df.columns[18:28])  # BP data\n",
    "    .union(df.columns[30:43])  # End game data columns\n",
    "    .union(df.columns[48:57])  # Drake-related columns\n",
    "    .union(df.columns[40:43])  # Individual data columns\n",
    "    .union(pd.Index([df.columns[78]]))  # Specific column (damageshare)\n",
    "    .union(pd.Index([df.columns[91]]))  # Specific column (earnedgoldshare)\n",
    "    .union(pd.Index([df.columns[95]]))  # Specific column (total cs)\n",
    "    .union(pd.Index([df.columns[28]]))  # Specific column (gamelength)\n",
    "    .union(df.columns[131:161])  # Data after 20 minutes\n",
    ")\n",
    "\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the count of null values in each column\n",
    "print(\"Null values in each column:\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_columns = null_counts[null_counts > 0]\n",
    "print(null_columns)\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "# Calculate the ratio of null values for each column\n",
    "null_ratio = (null_counts / total_rows)\n",
    "\n",
    "# Filter and print only the columns where the ratio of null values is greater than 0\n",
    "null_columns_with_ratio = null_ratio[null_ratio > 0]\n",
    "print(\"Columns with null values and their ratios:\")\n",
    "print(null_columns_with_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop or fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['void_grubs'] = df['void_grubs'].fillna(0)\n",
    "df['opp_void_grubs'] = df['opp_void_grubs'].fillna(0)\n",
    "df['turretplates'] = df['turretplates'].fillna(0)\n",
    "df['opp_turretplates'] = df['opp_turretplates'].fillna(0)\n",
    "df['heralds'] = df['heralds'].fillna(0)\n",
    "df['opp_heralds'] = df['opp_heralds'].fillna(0)\n",
    "\n",
    "\n",
    "df['cspm'] = df['cspm'].fillna(df['cspm'].median())\n",
    "df['vspm'] = df['vspm'].fillna(df['vspm'].median())\n",
    "df['visionscore'] = df['visionscore'].fillna(df['visionscore'].median())\n",
    "df.drop(columns=['monsterkillsownjungle', 'monsterkillsenemyjungle'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.drop(columns=['gameid', 'side'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify That All Missing Values Are Handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the count of null values in each column\n",
    "print(\"Null values in each column:\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_columns = null_counts[null_counts > 0]\n",
    "print(null_columns)\n",
    "print(\"\")\n",
    "\n",
    "# Calculate the ratio of null values for each column\n",
    "null_ratio = (null_counts / total_rows)\n",
    "\n",
    "# Filter and print only the columns where the ratio of null values is greater than 0\n",
    "null_columns_with_ratio = null_ratio[null_ratio > 0]\n",
    "print(\"Columns with null values and their ratios:\")\n",
    "print(null_columns_with_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Variables into Numerical Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "champ_cols = ['top_champ', 'jng_champ', 'mid_champ', 'bot_champ', 'sup_champ']\n",
    "\n",
    "all_champs = pd.concat([df[col] for col in champ_cols], axis=0).unique()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(all_champs)\n",
    "\n",
    "for col in champ_cols:\n",
    "    df[col] = le.transform(df[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save champion-label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "champ_mapping = pd.DataFrame({\n",
    "    \"champion\": le.classes_,\n",
    "    \"label\": le.transform(le.classes_)\n",
    "})\n",
    "\n",
    "champ_mapping_path = \"data/champion_label_mapping.csv\"\n",
    "\n",
    "\n",
    "champ_mapping.to_csv(champ_mapping_path, index=False)\n",
    "print(f\"üìù Champion-label mapping saved to: {champ_mapping_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the head and tail of the dataframe\n",
    "head_and_tail = pd.concat([df.head(), df.tail()])\n",
    "\n",
    "display(pd.concat([df.head(), df.tail()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts = df['result'].value_counts()\n",
    "display(class_counts)\n",
    "\n",
    "df['result'].value_counts(normalize=True).plot(kind='bar', title='Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "print(\"Feature skewness: \")\n",
    "display(df.skew().sort_values(ascending=False))\n",
    "\n",
    "skewed = df.skew()[abs(df.skew()) > 1].index\n",
    "axes = df.hist(bins=30, figsize=(25, 20))\n",
    "\n",
    "# Annotate skewed features\n",
    "for ax in axes.flatten():\n",
    "    if ax.get_title() in skewed:\n",
    "        ax.set_title(ax.get_title(), color='red')\n",
    "\n",
    "plt.suptitle(\"Feature Distributions (Red = Skewed)\", fontsize=20)\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply normalization to skew data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Compute skewed features\n",
    "skewed_features = df.skew()[abs(df.skew()) >= 1].index.tolist()\n",
    "\n",
    "# Apply log transform to skewed + non-negative features\n",
    "for col in skewed_features:\n",
    "    if (df[col] >= 0).all():\n",
    "        df[col] = np.log1p(df[col])\n",
    "        df.rename(columns={col: f\"{col}_normalized\"}, inplace=True)\n",
    "    else:\n",
    "        print(f\"Feature {col} contains negative values, skip log transform\")\n",
    "\n",
    "# Define which columns to exclude from scaling\n",
    "label_col = 'result'\n",
    "binary_cols = [\n",
    "    'firstdragon', 'firstherald', 'firstbaron', 'firsttower',\n",
    "    'firstmidtower', 'firsttothreetowers'\n",
    "]\n",
    "categorical_cols = [\n",
    "    'top_champ', 'jng_champ', 'mid_champ', 'bot_champ', 'sup_champ'\n",
    "]\n",
    "exclude_cols = [label_col] + binary_cols + categorical_cols\n",
    "normalize_cols = df.columns.difference(exclude_cols)\n",
    "\n",
    "# Fit and apply MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[normalize_cols] = scaler.fit_transform(df[normalize_cols])\n",
    "\n",
    "# Save transformer and info\n",
    "joblib.dump(scaler, \"data/minmax_scaler.pkl\")\n",
    "with open(\"data/skewed_features.json\", \"w\") as f:\n",
    "    json.dump(skewed_features, f)\n",
    "with open(\"data/normalize_cols.json\", \"w\") as f:\n",
    "    json.dump(list(normalize_cols), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Distribution after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "\n",
    "axes = df.hist(bins=30, figsize=(25, 20))\n",
    "\n",
    "plt.suptitle(\"Feature Distributions after normalization\", fontsize=20)\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_means = df.groupby('result').mean()\n",
    "display(grouped_means)\n",
    "\n",
    "df.groupby('result').mean().T.plot(kind='bar', figsize=(25, 20), title='Feature Mean by Result')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Matrix / Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# correlation_matrix = df.corr()\n",
    "# display(correlation_matrix)\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', center=0, annot=False)\n",
    "plt.title('Correlation Heatmap', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the processed data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/processed_lol_data.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_numerical = [\n",
    "    True,  # team kpm\n",
    "    True,  # ckpm\n",
    "    False, # firstdragon\n",
    "    True,  # dragons\n",
    "    True,  # opp_dragons\n",
    "    True,  # elders_normalized\n",
    "    True,  # opp_elders_normalized\n",
    "    False, # firstherald\n",
    "    True,  # heralds\n",
    "    True,  # opp_heralds\n",
    "    True,  # void_grubs_normalized\n",
    "    True,  # opp_void_grubs_normalized\n",
    "    False, # firstbaron\n",
    "    True,  # barons\n",
    "    True,  # opp_barons\n",
    "    False, # firsttower\n",
    "    True,  # towers\n",
    "    True,  # opp_towers\n",
    "    False, # firstmidtower\n",
    "    False, # firsttothreetowers\n",
    "    True,  # turretplates\n",
    "    True,  # opp_turretplates\n",
    "    True,  # inhibitors_normalized\n",
    "    True,  # opp_inhibitors_normalized\n",
    "    True,  # damagetochampions\n",
    "    True,  # dpm\n",
    "    True,  # damagetakenperminute\n",
    "    True,  # damagemitigatedperminute_normalized\n",
    "    True,  # wardsplaced\n",
    "    True,  # wpm\n",
    "    True,  # wardskilled\n",
    "    True,  # wcpm\n",
    "    True,  # controlwardsbought\n",
    "    True,  # visionscore\n",
    "    True,  # vspm\n",
    "    True,  # totalgold\n",
    "    True,  # earnedgold\n",
    "    True,  # earned gpm\n",
    "    True,  # goldspent\n",
    "    True,  # gspd\n",
    "    True,  # gpr\n",
    "    True,  # minionkills\n",
    "    True,  # monsterkills\n",
    "    True,  # cspm\n",
    "    True,  # goldat10\n",
    "    True,  # xpat10\n",
    "    True,  # csat10\n",
    "    True,  # opp_goldat10\n",
    "    True,  # opp_xpat10\n",
    "    True,  # opp_csat10\n",
    "    True,  # golddiffat10\n",
    "    True,  # xpdiffat10\n",
    "    True,  # csdiffat10\n",
    "    True,  # killsat10_normalized\n",
    "    True,  # assistsat10_normalized\n",
    "    True,  # deathsat10_normalized\n",
    "    True,  # opp_killsat10_normalized\n",
    "    True,  # opp_assistsat10_normalized\n",
    "    True,  # opp_deathsat10_normalized\n",
    "    True,  # goldat15\n",
    "    True,  # xpat15\n",
    "    True,  # csat15\n",
    "    True,  # opp_goldat15\n",
    "    True,  # opp_xpat15\n",
    "    True,  # opp_csat15\n",
    "    True,  # golddiffat15\n",
    "    True,  # xpdiffat15\n",
    "    True,  # csdiffat15\n",
    "    True,  # killsat15_normalized\n",
    "    True,  # assistsat15_normalized\n",
    "    True,  # deathsat15_normalized\n",
    "    True,  # opp_killsat15_normalized\n",
    "    True,  # opp_assistsat15_normalized\n",
    "    True,  # opp_deathsat15_normalized\n",
    "    False, # top_champ\n",
    "    False, # jng_champ\n",
    "    False, # mid_champ\n",
    "    False, # bot_champ\n",
    "    False  # sup_champ\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = 'GPU-1286b34f-f961-103e-a36e-052f44e4ac51'\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, batch_size=64, test_size=0.2, seed=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torch\n",
    "\n",
    "    class LoLDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, df):\n",
    "            self.X = torch.tensor(df.drop(columns=['result']).values, dtype=torch.float32)\n",
    "            self.y = torch.tensor(df['result'].values, dtype=torch.long)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df['result'])\n",
    "\n",
    "    train_dataset = LoLDataset(train_df)\n",
    "    test_dataset = LoLDataset(test_df)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from lolnet import LoLNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10, device='cpu', patience=3):\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    best_acc = 0\n",
    "    best_epoch = -1\n",
    "    epoch_log = []\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        model.eval()\n",
    "        test_preds, test_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        test_acc = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "        print(f\"üß™ Epoch {epoch+1:>2}/{num_epochs:<2} | \"\n",
    "              f\"Loss: {avg_loss:10.4f} | \"\n",
    "              f\"Train Acc: {train_acc:7.4f} | \"\n",
    "              f\"Test Acc: {test_acc:7.4f}\")\n",
    "\n",
    "        epoch_log.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'loss': avg_loss\n",
    "        })\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()\n",
    "        elif epoch - best_epoch >= patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, best_acc, epoch_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Loop with Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def add_input_noise(x, sigma=0.01, is_numerical=None):\n",
    "    noise = torch.randn_like(x) * sigma\n",
    "    if is_numerical is not None:\n",
    "        mask = torch.tensor(is_numerical, dtype=torch.float32, device=x.device)\n",
    "        noise = noise * mask\n",
    "    return torch.clamp(x + noise, 0.0, 1.0)\n",
    "\n",
    "def generate_pgd_adversarial(model, x, y, epsilon=0.02, alpha=0.002, steps=10, is_numerical=None):\n",
    "    x_adv = x.clone().detach().requires_grad_(True)\n",
    "    mask = torch.tensor(is_numerical, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        outputs = model(x_adv)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, y)\n",
    "        grads = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "        grads = grads.sign() * mask\n",
    "\n",
    "        x_adv = x_adv + alpha * grads.sign()\n",
    "        x_adv = torch.min(torch.max(x_adv, x - epsilon), x + epsilon)\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0).detach().requires_grad_(True)\n",
    "    \n",
    "    return x_adv.detach()\n",
    "\n",
    "def train_model_with_perturbation(\n",
    "    model, train_loader, test_loader, criterion, optimizer,\n",
    "    num_epochs=10, device='cpu', patience=3, mode='pgd', is_numerical=None,\n",
    "    pgd_config={'epsilon': 0.02, 'alpha': 0.002, 'steps': 10}, noise_sigma=0.01\n",
    "):\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    best_acc = 0\n",
    "    best_epoch = -1\n",
    "    epoch_log = []\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            if mode == 'pgd':\n",
    "                X_batch = generate_pgd_adversarial(model, X_batch, y_batch,\n",
    "                                                   epsilon=pgd_config['epsilon'],\n",
    "                                                   alpha=pgd_config['alpha'],\n",
    "                                                   steps=pgd_config['steps'],\n",
    "                                                   is_numerical=is_numerical)\n",
    "            elif mode == 'noise':\n",
    "                X_batch = add_input_noise(X_batch, sigma=noise_sigma, is_numerical=is_numerical)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_preds, test_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        test_acc = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "        print(f\"üß™ Epoch {epoch+1:>2}/{num_epochs:<2} | \"\n",
    "              f\"Loss: {avg_loss:10.4f} | \"\n",
    "              f\"Train Acc: {train_acc:7.4f} | \"\n",
    "              f\"Test Acc: {test_acc:7.4f}\")\n",
    "\n",
    "        epoch_log.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'loss': avg_loss\n",
    "        })\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()\n",
    "        elif epoch - best_epoch >= patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, best_acc, epoch_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_lolnet_model(df, model_name, is_numerical, do_train=False, train_mode='clean'):\n",
    "    if not do_train:\n",
    "        print(f\"‚ö†Ô∏è Skipping training as {do_train = }\")\n",
    "        return\n",
    "\n",
    "    assert train_mode in ['clean', 'pgd', 'noise'], f\"Invalid train_mode: {train_mode}\"\n",
    "    print(f\"üéØ Selected training mode: {train_mode}\")\n",
    "\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    num_epochs = 30\n",
    "    patience = 10\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"üöÄ Training with {device = }\")\n",
    "\n",
    "    train_loader, test_loader = create_dataloaders(df, batch_size=batch_size)\n",
    "    input_dim = df.drop(columns=['result']).shape[1]\n",
    "\n",
    "    model = LoLNet(input_dim=input_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training\n",
    "    if train_mode == 'clean':\n",
    "        model, best_acc, epoch_log = train_model(\n",
    "            model, train_loader, test_loader,\n",
    "            criterion, optimizer,\n",
    "            num_epochs=num_epochs, device=device,\n",
    "            patience=patience\n",
    "        )\n",
    "    else:\n",
    "        model, best_acc, epoch_log = train_model_with_perturbation(\n",
    "            model, train_loader, test_loader,\n",
    "            criterion, optimizer,\n",
    "            num_epochs=num_epochs, device=device,\n",
    "            patience=patience,\n",
    "            mode=train_mode,\n",
    "            is_numerical=is_numerical,\n",
    "            pgd_config={ 'epsilon': 0.02,'alpha': 0.002,'steps': 10 },\n",
    "            noise_sigma=0.01\n",
    "        )\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    model_path = f\"models/{model_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"‚úÖ Training complete. Best test accuracy: {best_acc:.4f}\")\n",
    "    print(f\"üíæ Model saved to: {model_path}\")\n",
    "\n",
    "    dummy_input = torch.randn(1, input_dim).to(device)\n",
    "    onnx_path = f\"models/{model_name}.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input,),\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "        opset_version=11\n",
    "    )\n",
    "    print(f\"üß† ONNX model exported to: {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping training as do_train = False\n",
      "üéØ Selected training mode: pgd\n",
      "üöÄ Training with device = 'cuda'\n",
      "üß™ Epoch  1/30 | Loss:     0.0973 | Train Acc:  0.9636 | Test Acc:  0.5154\n",
      "üß™ Epoch  2/30 | Loss:     0.0651 | Train Acc:  0.9753 | Test Acc:  0.5214\n",
      "üß™ Epoch  3/30 | Loss:     0.0572 | Train Acc:  0.9769 | Test Acc:  0.5263\n",
      "üß™ Epoch  4/30 | Loss:     0.0522 | Train Acc:  0.9777 | Test Acc:  0.5228\n",
      "üß™ Epoch  5/30 | Loss:     0.0481 | Train Acc:  0.9780 | Test Acc:  0.5351\n",
      "üß™ Epoch  6/30 | Loss:     0.0442 | Train Acc:  0.9787 | Test Acc:  0.5295\n",
      "üß™ Epoch  7/30 | Loss:     0.0421 | Train Acc:  0.9794 | Test Acc:  0.5215\n",
      "üß™ Epoch  8/30 | Loss:     0.0415 | Train Acc:  0.9791 | Test Acc:  0.5228\n",
      "üß™ Epoch  9/30 | Loss:     0.0405 | Train Acc:  0.9800 | Test Acc:  0.5240\n",
      "üß™ Epoch 10/30 | Loss:     0.0401 | Train Acc:  0.9800 | Test Acc:  0.5236\n",
      "üß™ Epoch 11/30 | Loss:     0.0396 | Train Acc:  0.9803 | Test Acc:  0.5336\n",
      "üß™ Epoch 12/30 | Loss:     0.0395 | Train Acc:  0.9804 | Test Acc:  0.5346\n",
      "üß™ Epoch 13/30 | Loss:     0.0389 | Train Acc:  0.9805 | Test Acc:  0.5246\n",
      "üß™ Epoch 14/30 | Loss:     0.0390 | Train Acc:  0.9808 | Test Acc:  0.5354\n",
      "üß™ Epoch 15/30 | Loss:     0.0387 | Train Acc:  0.9807 | Test Acc:  0.5440\n",
      "üß™ Epoch 16/30 | Loss:     0.0385 | Train Acc:  0.9805 | Test Acc:  0.5451\n",
      "üß™ Epoch 17/30 | Loss:     0.0381 | Train Acc:  0.9812 | Test Acc:  0.5474\n",
      "üß™ Epoch 18/30 | Loss:     0.0383 | Train Acc:  0.9808 | Test Acc:  0.5561\n",
      "üß™ Epoch 19/30 | Loss:     0.0380 | Train Acc:  0.9807 | Test Acc:  0.5576\n",
      "üß™ Epoch 20/30 | Loss:     0.0377 | Train Acc:  0.9809 | Test Acc:  0.5577\n",
      "üß™ Epoch 21/30 | Loss:     0.0378 | Train Acc:  0.9814 | Test Acc:  0.5563\n",
      "üß™ Epoch 22/30 | Loss:     0.0377 | Train Acc:  0.9811 | Test Acc:  0.5563\n",
      "üß™ Epoch 23/30 | Loss:     0.0370 | Train Acc:  0.9813 | Test Acc:  0.5521\n",
      "üß™ Epoch 24/30 | Loss:     0.0378 | Train Acc:  0.9811 | Test Acc:  0.5584\n",
      "üß™ Epoch 25/30 | Loss:     0.0374 | Train Acc:  0.9813 | Test Acc:  0.5387\n",
      "üß™ Epoch 26/30 | Loss:     0.0376 | Train Acc:  0.9812 | Test Acc:  0.5664\n",
      "üß™ Epoch 27/30 | Loss:     0.0369 | Train Acc:  0.9816 | Test Acc:  0.5665\n",
      "üß™ Epoch 28/30 | Loss:     0.0371 | Train Acc:  0.9811 | Test Acc:  0.5622\n",
      "üß™ Epoch 29/30 | Loss:     0.0372 | Train Acc:  0.9813 | Test Acc:  0.5696\n",
      "üß™ Epoch 30/30 | Loss:     0.0371 | Train Acc:  0.9815 | Test Acc:  0.5702\n",
      "‚úÖ Training complete. Best test accuracy: 0.5702\n",
      "üíæ Model saved to: models/lolnet_pgd.pth\n",
      "üß† ONNX model exported to: models/lolnet_pgd.onnx\n",
      "üéØ Selected training mode: noise\n",
      "üöÄ Training with device = 'cuda'\n",
      "üß™ Epoch  1/30 | Loss:     0.0676 | Train Acc:  0.9749 | Test Acc:  0.5291\n",
      "üß™ Epoch  2/30 | Loss:     0.0344 | Train Acc:  0.9863 | Test Acc:  0.5583\n",
      "üß™ Epoch  3/30 | Loss:     0.0294 | Train Acc:  0.9883 | Test Acc:  0.5575\n",
      "üß™ Epoch  4/30 | Loss:     0.0256 | Train Acc:  0.9898 | Test Acc:  0.5532\n",
      "üß™ Epoch  5/30 | Loss:     0.0227 | Train Acc:  0.9912 | Test Acc:  0.5262\n",
      "üß™ Epoch  6/30 | Loss:     0.0202 | Train Acc:  0.9922 | Test Acc:  0.5101\n",
      "üß™ Epoch  7/30 | Loss:     0.0187 | Train Acc:  0.9923 | Test Acc:  0.5093\n",
      "üß™ Epoch  8/30 | Loss:     0.0189 | Train Acc:  0.9924 | Test Acc:  0.5061\n",
      "üß™ Epoch  9/30 | Loss:     0.0182 | Train Acc:  0.9927 | Test Acc:  0.5063\n",
      "üß™ Epoch 10/30 | Loss:     0.0176 | Train Acc:  0.9930 | Test Acc:  0.5056\n",
      "üß™ Epoch 11/30 | Loss:     0.0173 | Train Acc:  0.9930 | Test Acc:  0.5064\n",
      "üß™ Epoch 12/30 | Loss:     0.0161 | Train Acc:  0.9933 | Test Acc:  0.5042\n",
      "‚èπÔ∏è Early stopping at epoch 12 (no improvement for 10 epochs)\n",
      "‚úÖ Training complete. Best test accuracy: 0.5583\n",
      "üíæ Model saved to: models/lolnet_noise.pth\n",
      "üß† ONNX model exported to: models/lolnet_noise.onnx\n"
     ]
    }
   ],
   "source": [
    "train_lolnet_model(df, \"lolnet\", is_numerical, do_train=False, train_mode='clean')\n",
    "train_lolnet_model(df, \"lolnet_pgd\", is_numerical, do_train=True, train_mode='pgd')\n",
    "train_lolnet_model(df, \"lolnet_noise\", is_numerical, do_train=True, train_mode='noise')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_numerical = [\n",
    "    True,  # team kpm\n",
    "    True,  # ckpm\n",
    "    False, # firstdragon\n",
    "    True,  # dragons\n",
    "    True,  # opp_dragons\n",
    "    True,  # elders_normalized\n",
    "    True,  # opp_elders_normalized\n",
    "    False, # firstherald\n",
    "    True,  # heralds\n",
    "    True,  # opp_heralds\n",
    "    True,  # void_grubs_normalized\n",
    "    True,  # opp_void_grubs_normalized\n",
    "    False, # firstbaron\n",
    "    True,  # barons\n",
    "    True,  # opp_barons\n",
    "    False, # firsttower\n",
    "    True,  # towers\n",
    "    True,  # opp_towers\n",
    "    False, # firstmidtower\n",
    "    False, # firsttothreetowers\n",
    "    True,  # turretplates\n",
    "    True,  # opp_turretplates\n",
    "    True,  # inhibitors_normalized\n",
    "    True,  # opp_inhibitors_normalized\n",
    "    True,  # damagetochampions\n",
    "    True,  # dpm\n",
    "    True,  # damagetakenperminute\n",
    "    True,  # damagemitigatedperminute_normalized\n",
    "    True,  # wardsplaced\n",
    "    True,  # wpm\n",
    "    True,  # wardskilled\n",
    "    True,  # wcpm\n",
    "    True,  # controlwardsbought\n",
    "    True,  # visionscore\n",
    "    True,  # vspm\n",
    "    True,  # totalgold\n",
    "    True,  # earnedgold\n",
    "    True,  # earned gpm\n",
    "    True,  # goldspent\n",
    "    True,  # gspd\n",
    "    True,  # gpr\n",
    "    True,  # minionkills\n",
    "    True,  # monsterkills\n",
    "    True,  # cspm\n",
    "    True,  # goldat10\n",
    "    True,  # xpat10\n",
    "    True,  # csat10\n",
    "    True,  # opp_goldat10\n",
    "    True,  # opp_xpat10\n",
    "    True,  # opp_csat10\n",
    "    True,  # golddiffat10\n",
    "    True,  # xpdiffat10\n",
    "    True,  # csdiffat10\n",
    "    True,  # killsat10_normalized\n",
    "    True,  # assistsat10_normalized\n",
    "    True,  # deathsat10_normalized\n",
    "    True,  # opp_killsat10_normalized\n",
    "    True,  # opp_assistsat10_normalized\n",
    "    True,  # opp_deathsat10_normalized\n",
    "    True,  # goldat15\n",
    "    True,  # xpat15\n",
    "    True,  # csat15\n",
    "    True,  # opp_goldat15\n",
    "    True,  # opp_xpat15\n",
    "    True,  # opp_csat15\n",
    "    True,  # golddiffat15\n",
    "    True,  # xpdiffat15\n",
    "    True,  # csdiffat15\n",
    "    True,  # killsat15_normalized\n",
    "    True,  # assistsat15_normalized\n",
    "    True,  # deathsat15_normalized\n",
    "    True,  # opp_killsat15_normalized\n",
    "    True,  # opp_assistsat15_normalized\n",
    "    True,  # opp_deathsat15_normalized\n",
    "    False, # top_champ\n",
    "    False, # jng_champ\n",
    "    False, # mid_champ\n",
    "    False, # bot_champ\n",
    "    False  # sup_champ\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Verified Robustness Rate Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maraboupy import Marabou\n",
    "from maraboupy.MarabouNetworkONNX import MarabouNetworkONNX\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import sys\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    " \n",
    "\n",
    "def test_lolnet_verified_robustness_rate(\n",
    "    df: pd.DataFrame,\n",
    "    onnx_path: str,\n",
    "    epsilon: List[float],\n",
    "    num_samples: int = 100,\n",
    "    verbosity: int = 1\n",
    "):\n",
    "    options = Marabou.createOptions(verbosity=0)\n",
    "    network: MarabouNetworkONNX = Marabou.read_onnx(onnx_path)\n",
    "\n",
    "    inputVars = network.inputVars[0][0]\n",
    "    outputVars = network.outputVars[0][0]\n",
    "\n",
    "    \n",
    "    if num_samples == -1:\n",
    "        samples = df.iterrows()\n",
    "    else:\n",
    "        assert num_samples <= len(df), \"Number of samples exceeds the size of the dataframe.\"\n",
    "        samples = df.sample(n=num_samples, random_state=42).iterrows()\n",
    "    \n",
    "    input_dim = len(inputVars)\n",
    "    results = []\n",
    "    \n",
    "    samples = list(samples)\n",
    "\n",
    "    for idx, row in tqdm(samples, desc=\"Verifying\", unit=\"sample\", leave=True):\n",
    "        x0 = row[1:].values.tolist()\n",
    "        true_label = int(row.iloc[0])\n",
    "\n",
    "        assert len(x0) == input_dim, \"Input dimension mismatch.\"\n",
    "\n",
    "        # Set input bounds\n",
    "        for i, x_i in enumerate(x0):\n",
    "            eps_i = epsilon[i]\n",
    "            network.setLowerBound(inputVars[i], x_i - eps_i)\n",
    "            network.setUpperBound(inputVars[i], x_i + eps_i)\n",
    "\n",
    "\n",
    "        # Set output constraint (y0 ‚â• y1 or y1 ‚â• y0)\n",
    "        y0, y1 = outputVars[0], outputVars[1]\n",
    "        if true_label == 0:\n",
    "            network.addInequality([y1, y0], [1, -1], 0) \n",
    "        else:\n",
    "            network.addInequality([y0, y1], [1, -1], 0) \n",
    "\n",
    "        # Solve\n",
    "        # with suppress_stdout():\n",
    "        #     status, assignments, stats = network.solve(options=options)\n",
    "        # status, assignments, stats = network.solve(options=options)\n",
    "        \n",
    "        orig_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        try:\n",
    "            status, assignments, stats = network.solve(options=options)\n",
    "        finally:\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = orig_stdout\n",
    "\n",
    "\n",
    "        results.append((idx, true_label, status))\n",
    "        \n",
    "        if verbosity == 1:\n",
    "            if status == \"sat\":\n",
    "                print(f\"‚ö†Ô∏è SAT ‚Äì idx {idx} | label: {true_label}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ UNSAT ‚Äì idx {idx} | label: {true_label}\")\n",
    "        elif verbosity == 2:\n",
    "            if status == \"sat\":\n",
    "                print(f\"‚ö†Ô∏è SAT ‚Äì idx {idx} | label: {true_label}\")\n",
    "                for i, var in enumerate(inputVars):\n",
    "                    val = assignments.get(var, None)\n",
    "                    try:\n",
    "                        print(f\"  x{i}: {float(val):.5f}\")\n",
    "                    except:\n",
    "                        print(f\"  x{i}: {val}\")\n",
    "\n",
    "                for j, out_var in enumerate(outputVars):\n",
    "                    val = assignments.get(out_var, None)\n",
    "                    try:\n",
    "                        print(f\"  y{j}: {float(val):.5f}\")\n",
    "                    except:\n",
    "                        print(f\"  y{j}: {val}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ UNSAT ‚Äì idx {idx} | label: {true_label}\")\n",
    "\n",
    "    # Print global summary for verbosity 0\n",
    "\n",
    "    num_total = len(results)\n",
    "    num_sat = sum(1 for r in results if r[2] == \"sat\")\n",
    "    num_unsat = num_total - num_sat\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Total samples tested: {num_total}\")\n",
    "    print(f\"  SAT (adversarial found): {num_sat}\")\n",
    "    print(f\"  UNSAT (robust): {num_unsat}\")\n",
    "    \n",
    "    return {\n",
    "        \"total_samples\": num_total,\n",
    "        \"num_verified_robust\": num_unsat,\n",
    "        \"num_adversarial_found\": num_sat,\n",
    "        \"verified_robustness_rate\": num_unsat / num_total,\n",
    "        \"results\": results  # optional\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define relative epsilon function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_epsilon(df: pd.DataFrame, is_numerical: List[bool], alpha: float = 0.01) -> List[float]:\n",
    "    epsilon_list = []\n",
    "\n",
    "    assert len(df.columns[1:]) == len(is_numerical), \"Length mismatch between columns and is_numerical list\"\n",
    "\n",
    "    for _, is_num in zip(df.columns[1:], is_numerical):\n",
    "        epsilon = alpha if is_num else 0.0\n",
    "        epsilon_list.append(epsilon)\n",
    "\n",
    "    return epsilon_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Robustness Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def test_lolnet_robustness_accuracy(\n",
    "    model,\n",
    "    df,\n",
    "    is_numerical,\n",
    "    epsilon_val=0.01,\n",
    "    batch_size=64,\n",
    "    alpha=0.005,\n",
    "    steps=20,\n",
    "    device='cuda'\n",
    "):\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    X = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)\n",
    "    y = torch.tensor(df.iloc[:, 0].values, dtype=torch.long)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    epsilon = torch.tensor(\n",
    "        [epsilon_val if is_numerical[i] else 0.0 for i in range(input_dim)],\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    mask = torch.tensor(is_numerical, dtype=torch.float32, device=device)\n",
    "\n",
    "    def masked_pgd(x, y):\n",
    "        x_orig = x.detach()\n",
    "        x_adv = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            outputs = model(x_adv)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, y)\n",
    "\n",
    "            grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "            grad = grad.sign() * mask  # Apply mask to preserve only numerical gradients\n",
    "            update = alpha * grad\n",
    "\n",
    "            x_adv = x_adv + update\n",
    "            x_adv = torch.min(torch.max(x_adv, x_orig - epsilon), x_orig + epsilon)\n",
    "            x_adv = torch.clamp(x_adv, 0.0, 1.0).detach().requires_grad_(True)\n",
    "\n",
    "        return x_adv.detach()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x_batch = X[i:i+batch_size].to(device)\n",
    "        y_batch = y[i:i+batch_size].to(device)\n",
    "\n",
    "        adv_x = masked_pgd(x_batch, y_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(adv_x).argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    robust_accuracy = correct / total\n",
    "    print(f\"üîê Robustness Accuracy: {robust_accuracy:.2%}\")\n",
    "    return robust_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Gradient Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def get_gradient_norms(model, df: pd.DataFrame, is_numerical: list[bool], norm_type=2, batch_size=64):\n",
    "    device = next(model.parameters()).device \n",
    "\n",
    "    X_test = df.drop(columns=[\"result\"]).values\n",
    "    mask = torch.tensor(is_numerical, dtype=torch.float32, device=device)\n",
    "    X_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "\n",
    "    def masked_gradient_norm(x):\n",
    "        x = x.clone().detach().requires_grad_(True)\n",
    "        outputs = model(x)\n",
    "        scores = outputs.max(1)[0]\n",
    "        grads = torch.autograd.grad(scores, x,\n",
    "                                    grad_outputs=torch.ones_like(scores),\n",
    "                                    retain_graph=False, create_graph=False)[0]\n",
    "        masked_grads = grads * mask\n",
    "        if norm_type == 'inf':\n",
    "            return masked_grads.abs().max(dim=1)[0]\n",
    "        return masked_grads.norm(p=norm_type, dim=1)\n",
    "\n",
    "    # Compute norms in batches\n",
    "    model.eval()\n",
    "    all_norms = []\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        x_batch = X_tensor[i:i + batch_size]\n",
    "        norms = masked_gradient_norm(x_batch)\n",
    "        all_norms.append(norms)\n",
    "\n",
    "    all_norms_tensor = torch.cat(all_norms)\n",
    "    print(f\"‚úÖ Gradient Norms ‚Äî Mean: {all_norms_tensor.mean():.4f} | Min: {all_norms_tensor.min():.4f} | Max: {all_norms_tensor.max():.4f}\")\n",
    "    return all_norms_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test verified robustness rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Verifying: lolnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:14<00:00, 13.47sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary:\n",
      "  Total samples tested: 1000\n",
      "  SAT (adversarial found): 228\n",
      "  UNSAT (robust): 772\n",
      "\n",
      "üß™ Verifying: lolnet_pgd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:24<00:00, 40.54sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary:\n",
      "  Total samples tested: 1000\n",
      "  SAT (adversarial found): 3\n",
      "  UNSAT (robust): 997\n",
      "\n",
      "üß™ Verifying: lolnet_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:24<00:00, 41.30sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary:\n",
      "  Total samples tested: 1000\n",
      "  SAT (adversarial found): 4\n",
      "  UNSAT (robust): 996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")\n",
    "_, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['result'])\n",
    "\n",
    "model_names = [\"lolnet\", \"lolnet_pgd\", \"lolnet_noise\"]\n",
    "epsilon = get_relative_epsilon(df, is_numerical, 0.05)\n",
    "\n",
    "for model_name in model_names:\n",
    "    onnx_path = f\"models/{model_name}.onnx\"\n",
    "    print(f\"\\nüß™ Verifying: {model_name}\")\n",
    "    test_lolnet_verified_robustness_rate(\n",
    "        test_df,\n",
    "        onnx_path=onnx_path,\n",
    "        epsilon=epsilon,\n",
    "        num_samples=1000,\n",
    "        verbosity=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Robustness Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Verifying: lolnet\n",
      "device = 'cuda'\n",
      "üîê Robustness Accuracy: 82.53%\n",
      "\n",
      "üß™ Verifying: lolnet_pgd\n",
      "device = 'cuda'\n",
      "üîê Robustness Accuracy: 96.83%\n",
      "\n",
      "üß™ Verifying: lolnet_noise\n",
      "device = 'cuda'\n",
      "üîê Robustness Accuracy: 79.38%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lolnet import LoLNet\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")\n",
    "_, test_df = train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['result'])\n",
    "\n",
    "input_dim = df.drop(columns=[\"result\"]).shape[1]\n",
    "\n",
    "model_names = [\"lolnet\", \"lolnet_pgd\", \"lolnet_noise\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_path = f\"models/{model_name}.pth\"\n",
    "    print(f\"\\nüß™ Verifying: {model_name}\")\n",
    "    model = LoLNet(input_dim=input_dim,hidden_dim=64)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"{device = }\")\n",
    "    model = LoLNet(input_dim=input_dim, hidden_dim=64).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    robust_accuracy = test_lolnet_robustness_accuracy(\n",
    "        model=model,\n",
    "        df=test_df,\n",
    "        is_numerical=is_numerical,\n",
    "        epsilon_val=0.05,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gradient Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Verifying: lolnet\n",
      "device = 'cuda'\n",
      "‚úÖ Gradient Norms ‚Äî Mean: 18.9923 | Min: 3.1303 | Max: 35.4360\n",
      "\n",
      "üß™ Verifying: lolnet_pgd\n",
      "device = 'cuda'\n",
      "‚úÖ Gradient Norms ‚Äî Mean: 28.8763 | Min: 22.6373 | Max: 30.4522\n",
      "\n",
      "üß™ Verifying: lolnet_noise\n",
      "device = 'cuda'\n",
      "‚úÖ Gradient Norms ‚Äî Mean: 23.5608 | Min: 13.9260 | Max: 29.5967\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lolnet import LoLNet\n",
    "\n",
    "df = pd.read_parquet(\"data/processed_lol_data.parquet\")\n",
    "_, test_df = train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['result'])\n",
    "\n",
    "input_dim = df.drop(columns=[\"result\"]).shape[1]\n",
    "\n",
    "model_names = [\"lolnet\", \"lolnet_pgd\", \"lolnet_noise\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_path = f\"models/{model_name}.pth\"\n",
    "    print(f\"\\nüß™ Verifying: {model_name}\")\n",
    "    model = LoLNet(input_dim=input_dim,hidden_dim=64)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"{device = }\")\n",
    "    model = LoLNet(input_dim=input_dim, hidden_dim=64).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    gradient_norm = get_gradient_norms(\n",
    "        model=model,\n",
    "        df=test_df,\n",
    "        is_numerical=is_numerical,    \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
